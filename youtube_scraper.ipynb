{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFYh0Tlg6XQzF3lwMLs8Hf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install yt-dlp youtube-transcript-api openai-whisper pytube requests\n"
      ],
      "metadata": {
        "id": "Rkr7H6fYSsnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet yt-dlp youtube-transcript-api requests openai-whisper\n",
        "!apt-get update -y && apt-get install -y ffmpeg\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "f8TXYYbzYEKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUUt6ycuFaa1",
        "outputId": "f807956f-129c-47df-ef02-387386b1435b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, time, random, requests, xml.etree.ElementTree as ET\n",
        "import yt_dlp, whisper\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
        "\n",
        "CHANNEL_ID   = \"UC-PaZZpjgJ61wkK9yKfpe8w\"\n",
        "OUTPUT_FILE  = \"/content/drive/MyDrive/transcripts.json\"\n",
        "PAUSE_MIN    = 2\n",
        "PAUSE_MAX    = 5\n",
        "\n",
        "print(\" Loading Whisper (small.en)…\")\n",
        "whisper_model = whisper.load_model(\"small.en\")\n",
        "\n",
        "def uploads_playlist_url(cid: str) -> str:\n",
        "    return f\"https://www.youtube.com/playlist?list=UU{cid[2:]}\"\n",
        "\n",
        "def list_video_ids(pl_url: str) -> list[str]:\n",
        "    opts = {\"extract_flat\": \"in_playlist\", \"skip_download\": True, \"quiet\": True}\n",
        "    with yt_dlp.YoutubeDL(opts) as ydl:\n",
        "        info = ydl.extract_info(pl_url, download=False)\n",
        "    return [\n",
        "        e[\"id\"] for e in info.get(\"entries\", [])\n",
        "        if isinstance(e.get(\"id\"), str) and len(e[\"id\"]) == 11\n",
        "    ]\n",
        "\n",
        "def fetch_api(vid: str) -> str | None:\n",
        "    try:\n",
        "        segs = YouTubeTranscriptApi.get_transcript(vid, languages=[\"en\"])\n",
        "        return \" \".join(s[\"text\"] for s in segs)\n",
        "    except (TranscriptsDisabled, NoTranscriptFound):\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def fetch_xml(vid: str) -> str | None:\n",
        "    url = f\"https://video.google.com/timedtext?lang=en&v={vid}\"\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        if not r.text.strip():\n",
        "            return None\n",
        "        root = ET.fromstring(f\"<root>{r.text}</root>\")\n",
        "        return \" \".join(node.text or \"\" for node in root.findall(\"text\"))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def fetch_whisper(vid: str) -> str | None:\n",
        "    out_fn = f\"{vid}.m4a\"\n",
        "    ydl_opts = {\n",
        "        \"format\": \"bestaudio[ext=m4a]/bestaudio\",\n",
        "        \"outtmpl\": out_fn,\n",
        "        \"quiet\": True\n",
        "    }\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([f\"https://youtu.be/{vid}\"])\n",
        "        result = whisper_model.transcribe(out_fn)\n",
        "        os.remove(out_fn)\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"     Whisper error for {vid}: {e}\")\n",
        "        if os.path.exists(out_fn):\n",
        "            os.remove(out_fn)\n",
        "        return None\n",
        "\n",
        "def scrape_all():\n",
        "    pl_url = uploads_playlist_url(CHANNEL_ID)\n",
        "    print(\"  Uploads playlist:\", pl_url)\n",
        "    vids = list_video_ids(pl_url)\n",
        "    print(f\"  Found {len(vids)} videos. Starting fetch…\\n\")\n",
        "\n",
        "    cache = {}\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        cache = json.load(open(OUTPUT_FILE))\n",
        "\n",
        "    for idx, vid in enumerate(vids, 1):\n",
        "        if vid in cache:\n",
        "            mark = \"yes\" if cache[vid] else \"no\"\n",
        "            print(f\"{idx}/{len(vids)} • {vid} … cached {mark}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"{idx}/{len(vids)} • {vid} …\", end=\"\", flush=True)\n",
        "        text = fetch_api(vid) or fetch_xml(vid) or fetch_whisper(vid)\n",
        "        cache[vid] = text or \"\"\n",
        "        print(\"yes\" if text else \"no\")\n",
        "\n",
        "        with open(OUTPUT_FILE, \"w\") as f:\n",
        "            json.dump(cache, f, indent=2)\n",
        "\n",
        "        time.sleep(random.uniform(PAUSE_MIN, PAUSE_MAX))\n",
        "\n",
        "    done = sum(1 for t in cache.values() if t)\n",
        "    print(f\"\\n Completed: {done}/{len(vids)} transcripts saved to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_all()\n"
      ],
      "metadata": {
        "id": "q5aj-YJnaTif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q yt-dlp pandas\n",
        "!apt-get update -y && apt-get install -y ffmpeg  # for yt-dlp’s internal checks\n"
      ],
      "metadata": {
        "id": "wgcVlMSQDcev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "import pandas as pd\n",
        "CHANNEL_ID    = \"UC-PaZZpjgJ61wkK9yKfpe8w\"\n",
        "PLAYLIST_URL  = f\"https://www.youtube.com/playlist?list=UU{CHANNEL_ID[2:]}\"\n",
        "ydl_opts = {\n",
        "    \"skip_download\":     True,\n",
        "    \"dump_single_json\":  True,\n",
        "    \"extract_flat\":      False,\n",
        "    \"quiet\":             True,\n",
        "}\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    playlist_info = ydl.extract_info(PLAYLIST_URL, download=False)\n",
        "playlist_id    = playlist_info.get(\"id\")\n",
        "playlist_title = playlist_info.get(\"title\")\n",
        "videos = playlist_info.get(\"entries\", [])\n",
        "print(f\"Fetched metadata for {len(videos)} videos from playlist '{playlist_title}'\")\n"
      ],
      "metadata": {
        "id": "bHb415yjDjAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean = []\n",
        "for entry in videos:\n",
        "    clean.append({\n",
        "        \"video_id\":      entry.get(\"id\"),\n",
        "        \"title\":         entry.get(\"title\"),\n",
        "        \"publish_date\":  entry.get(\"upload_date\"),\n",
        "        \"view_count\":    entry.get(\"view_count\"),\n",
        "        \"like_count\":    entry.get(\"like_count\"),\n",
        "        \"comment_count\": entry.get(\"comment_count\"),\n",
        "        \"description\":   entry.get(\"description\"),\n",
        "        \"tags\":          entry.get(\"tags\"),\n",
        "        \"thumbnail_url\": (entry.get(\"thumbnails\") or [{}])[-1].get(\"url\"),\n",
        "        \"duration\":      entry.get(\"duration\"),\n",
        "        \"video_url\":     entry.get(\"webpage_url\"),\n",
        "        \"channel_name\":  entry.get(\"uploader\"),\n",
        "        \"channel_id\":    entry.get(\"uploader_id\"),\n",
        "        \"playlist_id\":   playlist_id,\n",
        "        \"playlist_title\":playlist_title\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(clean)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "cZVrpfLKESvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"videos_clean_metadata.csv\", index=False)\n",
        "df.to_json(\"videos_clean_metadata.json\", orient=\"records\", indent=2)\n",
        "\n",
        "print(\"Saved videos_clean_metadata.csv & .json\")\n"
      ],
      "metadata": {
        "id": "dx4WEPZFFRJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "metadata_json_path    = \"/content/videos_clean_metadata.json\"\n",
        "transcripts_json_path = \"/content/drive/MyDrive/transcripts.json\"\n",
        "merged_json_path      = \"/content/drive/MyDrive/videos_merged.json\"\n",
        "merged_csv_path       = \"/content/drive/MyDrive/videos_merged.csv\"\n",
        "\n",
        "with open(METADATA_JSON, 'r') as f:\n",
        "    metadata_list = json.load(f)\n",
        "\n",
        "with open(TRANSCRIPTS_JSON, 'r') as f:\n",
        "    transcripts_dict = json.load(f)\n",
        "\n",
        "for entry in metadata_list:\n",
        "    vid = entry.get('id') or entry.get('video_id') \\\n",
        "          or entry.get('video_url', '').split('/')[-1]\n",
        "    entry['transcript'] = transcripts_dict.get(vid, \"\")\n",
        "with open(merged_json_path, 'w') as f:\n",
        "    json.dump(metadata_list, f, indent=2)\n",
        "df = pd.json_normalize(metadata_list)\n",
        "df.to_csv(merged_csv_path, index=False)\n",
        "\n",
        "print(f\" Merged data saved to:\\n  • {merged_json_path}\\n  • {merged_csv_path}\")"
      ],
      "metadata": {
        "id": "BTsEsHSDW8oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "INPUT_JSON  = \"/content/drive/MyDrive/videos_merged_normalized.json\"\n",
        "OUTPUT_JSON = \"/content/videos_cleaned.json\"\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"\\[?\\d{1,2}:\\d{2}(?::\\d{2})?\\]?\", \" \", text)\n",
        "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "cleaned = []\n",
        "for entry in data:\n",
        "    entry = entry.copy()\n",
        "    entry[\"transcript_clean\"] = clean_text(entry.get(\"transcript\", \"\"))\n",
        "    entry[\"description_clean\"] = clean_text(entry.get(\"description\", \"\"))\n",
        "    cleaned.append(entry)\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned, f, indent=2, ensure_ascii=False)\n",
        "print(f\"Saved cleaned transcripts and metadata to: {OUTPUT_JSON}\")\n"
      ],
      "metadata": {
        "id": "6pPZRRmyub12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "INPUT_JSON  = \"/content/videos_cleaned.json\"\n",
        "OUTPUT_JSON = \"/content/videos_chunked.json\"\n",
        "CHUNK_SIZE  = 1000\n",
        "OVERLAP     = 200\n",
        "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=OVERLAP,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        ")\n",
        "chunked_records = []\n",
        "for entry in data:\n",
        "    transcript = entry.get(\"transcript_clean\", \"\")\n",
        "    if not transcript.strip():\n",
        "        continue\n",
        "    chunks = splitter.split_text(transcript)\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        record = {k: v for k, v in entry.items() if k != \"transcript_clean\"}\n",
        "        record[\"chunk_index\"] = idx\n",
        "        record[\"chunk_text\"] = chunk\n",
        "        chunked_records.append(record)\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunked_records, f, indent=2, ensure_ascii=False)\n",
        "print(f\"Saved {len(chunked_records)} chunks to {OUTPUT_JSON}\")"
      ],
      "metadata": {
        "id": "FPgwtu9_5Ypm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "NhkqMGRN_X55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "CHUNKS_PATH  = \"/content/videos_chunked.json\"\n",
        "OUTPUT_PATH  = \"/content/chunks_with_embeddings.json\"\n",
        "EMBED_MODEL  = \"all-MiniLM-L6-v2\"\n",
        "BATCH_SIZE   = 64\n",
        "with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)\n",
        "texts = [c[\"chunk_text\"] for c in chunks]\n",
        "model = SentenceTransformer(EMBED_MODEL)\n",
        "embeddings = []\n",
        "for i in tqdm(range(0, len(texts), BATCH_SIZE)):\n",
        "    batch = texts[i:i+BATCH_SIZE]\n",
        "    batch_embeds = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)\n",
        "    embeddings.extend(batch_embeds)\n",
        "assert len(embeddings) == len(chunks)\n",
        "for rec, vec in zip(chunks, embeddings):\n",
        "    rec[\"embedding\"] = vec.tolist()\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
        "print(f\"Saved {len(chunks)} embedded chunks to {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "6761edca1afc4119bc2fc3845c111800",
            "ef91206046994a21b7bf8837b463e6ff",
            "956977fc890b4b0584c2a1ee7df3f818",
            "18b5ed0272c14617b44b1eff2eda0012",
            "3a44b29564da47089c9e20a2f2156017",
            "c88048b2d63340fc98facfc87cff239d",
            "98c3127f08b641e38d5feb8a23fb972a",
            "0f82fe6fa7024e039dd1061918f4f54e",
            "dc3f5a6f9197457cbaae6b8edbccc855",
            "e0829da1cda34608b5db2da7471f2df2",
            "1928bbb7dd6c459f9655ef4ca2afd87b",
            "74899c176a704783be268d03b24d203c",
            "cf42fa670bcb40ad81c2edff73c1e042",
            "b76e73b6a8174a72a8f65dc58ad015bf",
            "422ec3e07784401f9a3989bcac397a55",
            "1f5dc5a5c5314c18b311f677c4d9ac78",
            "efae58dd01744f719c0a1f6737cd3e5e",
            "1617b84887b24b7b9dac29e419b800c5",
            "543cc3e6c77347c4ab352e5e351ced6c",
            "67a902ede94d402391eaea89184ede80",
            "6c9662aa406d4e229ce0ea1746eec030",
            "115b3758c47646f08237b9574e83810f",
            "d3852abe755140febaf2e70354da5f10",
            "de7ea4e9581e4b669e94dc1e2632d60e",
            "b10fe41504ff4aa7be60ab15987074c2",
            "c5c9754e965e4c84ba7620c2a663ba02",
            "223383ae999646dc9c5ba1269d53ad50",
            "938e4bd5f28444c6b596f8ee9b3d9190",
            "0ddcbfeabbbb4ddcb9443acc5665b912",
            "4455d3752bc34f62813aac3a36ba6fa9",
            "0a2324e463174c6084a508959afa5cc6",
            "5d4d2852ec4049299e1bb9742650d0e0",
            "371e3924651d47bd994d9a7955adfe7a",
            "3cc59bcd513b4ae8a0592a08935455c9",
            "ff2ddc895d324ccaad8d4a308b7e04af",
            "bcd3b29f7ca143dcad5d155c3fad4452",
            "89577f481cb64fa1b40cd3170d5ecaea",
            "0115c06930a24e69b00e3e6641617f06",
            "972b9cd29746422fa62e0ffffe6d49a3",
            "ec145e7062a84139b7b6c13998239dde",
            "6f9afe6eed924be9b616a64799650319",
            "1ca46c6f112243f2bf8c38df971442c2",
            "bf25a0fe7dbb4d19ba72c4e27d8e2a43",
            "b9c180d7134140059845878b64a18fc4",
            "71714bedc55f43af98dc09e265627b6b",
            "9215b98467814b11b55fb5eea23d4dc0",
            "ae45e10c77f949eea32a393cc3fa0d6a",
            "a0efe448afee4347ab95231967dbc73e",
            "90d50062ecea47f69cbb67aefecb26e4",
            "f7d810e08faf4b47a2a1d0b411438ca2",
            "48c260d781ad43f0bdea6a5118f1c325",
            "9d95c82051ba49aab25f9a7c3ab4bcc2",
            "20b901dd544242b59cfecfc9ef5020af",
            "19448af2c4f14485b5425fe8de30437a",
            "3a75f50bba2143cc8d7b343a78b91760",
            "8b36ca1a38a7446982751f2118f3c3af",
            "6d92a704d9014603998759938e09dfe3",
            "cbee145d68a44234ad293bacf43f7a65",
            "b3d2a11c843e4b30bd7ad49a4c9c78d2",
            "ee8a01c236db415e95e1585e7aa3c27b",
            "8f49abb1ed194ec4a677ac35e72d3bee",
            "94022fad680f4adf8f117aae9a6521d8",
            "e6d71c2e628e41ac9f5e2bbc34acd5c5",
            "085158f01eeb41f2b32d5edfde0529c0",
            "38ddef9cdc584a5eaff011ce005e458d",
            "68069ab11c564c59b823029014fd8476",
            "530ad86f81904e7eb066eb5381d7bdbe",
            "dc07e015d30f45198a4730ef610ae8e5",
            "a416b2fb4bf84228a428579b09c87ed4",
            "9dcc7468a17b4949be8c0b36e8ce7b23",
            "d809007363a6451da361b9459c0e29e1",
            "11944a2747e245ffa300228c0d68a5d9",
            "523f36e293fb45b4b655bfeb0a31d5fc",
            "604dd452bcf34b5dbaaa9385aa1e5fd5",
            "9e5a12ff75ee49bf8712709fc4d6c684",
            "2f8e55042ff24c778215db6749c5f8bc",
            "d528475bb12b483c9d20a585ad3410cb",
            "8244f83e07c248af9e71daed2561b280",
            "00b8be6f127e40c0b6a8462a597efaf3",
            "140fb2f7b4254205bf2df6da1a487f58",
            "c803c8db9e294a8593f35da2c51554de",
            "07fc9e30f80c40ed80feb9080c51a574",
            "27cbe2aa11994071b96196e3473a5892",
            "20c9692d9470425289790964bf283bf9",
            "c7166b3e047b4a538bbc212ed54423a0",
            "ab6c43536a8440ce8464f5e708fcd4c4",
            "ccdcf57e51734d54898d45db25b49bd9",
            "36ff99b2640a451cbd90d3e69102dabd",
            "b88da0ddf9014132bc42fbf1129c27e0",
            "fdfe8b6f41344940aee0a81d097bea2c",
            "eba2a5130f10494d9442329963fb42ad",
            "018641f0fd2b4eefb2d97f3a44fec39b",
            "5f0d523888544741bf7d8f669532ed2f",
            "9884403395aa42e284bd4ab3714f1ab1",
            "75f3e185d26446ba93fc077b903e5ade",
            "b466dc7f6b7442d39589e772ee00bf6f",
            "40e977f421044c23ad692b9c5b8a517e",
            "54df106b7f7445b59916342f6f5b2373",
            "d1e5c9540ba94ef6bfadbb9d867fb128",
            "430b4bb90c724d7c87387bb175861c44",
            "1b7b0bf6209e4b6881434c8a7e6a212b",
            "44ae5bdfa5ea4d05a088cb1b41d27024",
            "147653e93974444cbda261924aa2a296",
            "4ee6173622f24df981ba86e921a9ec45",
            "fa4a050345534642a310f8126159ded1",
            "186b342fbd464a2cadc8d230c77a845d",
            "574cdc67bb734bd2878d0d5b935c831c",
            "21ead55f56804e3abaa8239ad3bd916b",
            "e1bc4c603cce4464b002071fbb440d35",
            "2e94218b78a646238ca31811e06cd997",
            "5e25c2ef12914d218080e85ab1455fb2",
            "154d5dd7fb264b178635f2008ec43032",
            "99330ba0b09a40b49032b3a6a0afad5b",
            "1aa9cbd4cf0c441f91cbeea5f7a72ede",
            "42163ce1f59649ebb249b1c5cda63bf5",
            "4e0d908615284c3eb55a460a7c137011",
            "f8f2ad2a17f34ae4a5cefe74ebd0ae7b",
            "300f2f7226eb4aa0804723d498500ce6",
            "0b2835e7c20d40da9ec1162feb09ac23",
            "e3ceb546672f4d3085c76f860e8c0d3d",
            "4592e4dc682e460b8721d8bf125f2388"
          ]
        },
        "id": "LXk2TnYyLiVb",
        "outputId": "24fee619-edba-428b-af99-c036664dc80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6761edca1afc4119bc2fc3845c111800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74899c176a704783be268d03b24d203c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3852abe755140febaf2e70354da5f10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cc59bcd513b4ae8a0592a08935455c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71714bedc55f43af98dc09e265627b6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b36ca1a38a7446982751f2118f3c3af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "530ad86f81904e7eb066eb5381d7bdbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8244f83e07c248af9e71daed2561b280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b88da0ddf9014132bc42fbf1129c27e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "430b4bb90c724d7c87387bb175861c44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e25c2ef12914d218080e85ab1455fb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:46<00:00,  8.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1274 embedded chunks to /content/chunks_with_embeddings.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op8uc5aEEy9A",
        "outputId": "01c0c7e9-8a1b-485a-a2d3-151ca12077d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "EMBEDDED_CHUNKS_JSON = \"/content/drive/MyDrive/chunks_with_embeddings.json\"\n",
        "FAISS_INDEX_PATH     = \"/content/faiss_index.index\"\n",
        "METADATA_JSON_PATH   = \"/content/faiss_metadata.json\"\n",
        "with open(EMBEDDED_CHUNKS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    records = json.load(f)\n",
        "embeddings = np.array([r[\"embedding\"] for r in records]).astype(\"float32\")\n",
        "metadata   = [ {k: v for k, v in r.items() if k != \"embedding\"} for r in records ]\n",
        "dim        = embeddings.shape[1]\n",
        "\n",
        "print(f\"Loaded {len(embeddings)} embeddings, dimension={dim}\")\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
        "faiss.write_index(index, FAISS_INDEX_PATH)\n",
        "with open(METADATA_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved FAISS index to: {FAISS_INDEX_PATH}\")\n",
        "print(f\"Saved metadata to:   {METADATA_JSON_PATH}\")\n",
        "index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "with open(METADATA_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    metadata = json.load(f)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "def search_faiss(query, top_k=5):\n",
        "    query_emb = model.encode([query]).astype(\"float32\")\n",
        "    D, I = index.search(query_emb, top_k)\n",
        "    results = []\n",
        "    for idx, dist in zip(I[0], D[0]):\n",
        "        result = metadata[idx].copy()\n",
        "        result[\"score\"] = float(dist)\n",
        "        results.append(result)\n",
        "    return results\n",
        "query = \"How do I grow my YouTube channel quickly?\"\n",
        "results = search_faiss(query, top_k=3)\n",
        "print(\"\\n--- Top Results ---\")\n",
        "for r in results:\n",
        "    print(f\"\\nTitle: {r.get('title')}\")\n",
        "    print(f\"Video URL: {r.get('video_url')}\")\n",
        "    print(f\"Chunk: {r.get('chunk_text')[:350]}...\")\n",
        "    print(f\"Distance: {r.get('score'):.4f}\")\n"
      ],
      "metadata": {
        "id": "FFKKsbqQHzTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai google-generativeai faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "id": "GJHRLPnkqSE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "FAISS_INDEX_PATH   = \"/content/faiss_index.index\"\n",
        "METADATA_JSON_PATH = \"/content/faiss_metadata.json\"\n",
        "EMBED_MODEL        = \"all-MiniLM-L6-v2\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"API key\"\n",
        "index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "with open(METADATA_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    metadata = json.load(f)\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
        "def search_faiss(query, top_k=4):\n",
        "    query_emb = embedder.encode([query]).astype(\"float32\")\n",
        "    D, I = index.search(query_emb, top_k)\n",
        "    results = []\n",
        "    for idx, dist in zip(I[0], D[0]):\n",
        "        rec = metadata[idx].copy()\n",
        "        rec[\"score\"] = float(dist)\n",
        "        results.append(rec)\n",
        "    return results\n",
        "def build_rag_prompt(context_chunks, user_question):\n",
        "    context_text = \"\"\n",
        "    for c in context_chunks:\n",
        "        context_text += (\n",
        "            f\"\\n---\\nTitle: {c.get('title','')}\\n\"\n",
        "            f\"URL: {c.get('video_url','')}\\n\"\n",
        "            f\"Excerpt: {c.get('chunk_text','')[:800]}...\\n\"\n",
        "        )\n",
        "    prompt = f\"\"\"You are an advanced YouTube Channel Growth Coach.\n",
        "Your job is to give creative, actionable, and specific advice based ONLY on the following video excerpts (with titles and URLs).\n",
        "\n",
        "{context_text}\n",
        "\n",
        "User's question:\n",
        "{user_question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Cite video titles/URLs as sources whenever possible.\n",
        "- If the user asks for a script or hook, write a full YouTube script/hook in the creator's style.\n",
        "- Be clear, step-by-step, and as practical as possible.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "print(\"YouTube Growth RAG Chatbot (type 'exit' to stop)\\n\")\n",
        "while True:\n",
        "    user_question = input(\"Your question: \")\n",
        "    if user_question.strip().lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    chunks = search_faiss(user_question, top_k=4)\n",
        "    prompt = build_rag_prompt(chunks, user_question)\n",
        "    print(\"\\n[Retrieving, reasoning...]\\n\")\n",
        "    response = llm.invoke(prompt)\n",
        "    print(\"\\n==== AI's Answer ====\\n\")\n",
        "    print(response.content)\n",
        "    print(\"\\n==== Source Videos ====\\n\")\n",
        "    for c in chunks:\n",
        "        print(f\"- {c.get('title')} ({c.get('video_url')}) [Score: {c.get('score'):.4f}]\")\n",
        "    print(\"\\n---\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c4541c-6dca-437a-f098-79dac0ca80c7",
        "id": "ViRVId3ZFrso"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YouTube Growth RAG Chatbot (type 'exit' to stop)\n",
            "\n",
            "Your question: how to create hooks for a new channel tech based\n",
            "\n",
            "[Retrieving, reasoning...]\n",
            "\n",
            "\n",
            "==== AI's Answer ====\n",
            "\n",
            "Creating compelling hooks for a new tech-based channel is crucial for capturing attention and attracting new viewers from the start. Here's a step-by-step approach based on the insights from these YouTube creators:\n",
            "\n",
            "### How to Create Hooks for a New Tech Channel\n",
            "\n",
            "**1. Research Niche-Specific Curiosity & Adjacent Channels (The \"Model 10\" for Tech)**\n",
            "\n",
            "*   **Identify Your Tech Niche's Core Questions:** Start by thinking about what problems your target audience in the tech space is trying to solve, what new technologies they're curious about, or what common misconceptions they have. The goal is to \"evoke curiosity\" immediately.\n",
            "*   **Analyze \"Adjacent Channels\":** As mentioned in *How to START & GROW a YouTube Channel in 2025* (https://www.youtube.com/watch?v=SXN9SMLWLzw), look beyond just direct competitors. If you're doing tech reviews, also look at channels that do \"how-to\" tech guides, tech news, or even channels in broader educational niches that explain complex topics simply. How do *they* open their videos to grab attention?\n",
            "    *   **Actionable:** Make a list of 5-10 successful channels (your \"Model 10\") in or adjacent to your tech niche. Watch the first 15-30 seconds of their most popular videos. What \"title formats\" or opening statements do they use that make you want to keep watching? Pay attention to:\n",
            "        *   Problem/Solution hooks (e.g., \"Struggling with slow Wi-Fi? This one setting changed everything!\")\n",
            "        *   Surprising fact/Myth-busting hooks (e.g., \"You've been charging your phone wrong this whole time.\")\n",
            "        *   Benefit-driven hooks (e.g., \"Here's how to make your old laptop feel brand new.\")\n",
            "        *   Question-based hooks (e.g., \"Is the new iPhone really worth the upgrade?\")\n",
            "\n",
            "**2. Leverage Keyword Research to Attract New Viewers**\n",
            "\n",
            "*   Since you're starting a new channel, your primary goal is to reach \"new people outside of [your] existing audience\" (*STARTING OVER ON YOUTUBE*, https://www.youtube.com/watch?v=b0iBT83zzBs). This means your hooks need to be self-contained and immediately valuable or intriguing to someone who's never seen your content before.\n",
            "*   **Integrate Search Intent:** The creator in *STARTING OVER ON YOUTUBE* mentions \"keyword research\" as a key launch tactic. This is vital for hooks. What are people *searching for* when they look for tech information? Your hook can directly address that search query or promise a solution to a popular tech problem.\n",
            "    *   **Actionable:** Use tools (even free ones like Google Trends, YouTube search suggestions, or AnswerThePublic) to find common questions or trending topics in your tech niche. Weave these keywords or the underlying problem/curiosity into your hook.\n",
            "\n",
            "**3. Use AI as a Brainstorming Partner for Iteration & Refinement**\n",
            "\n",
            "*   The process outlined in *How to Turn AI Into Your Personal YouTube Team* (https://www.youtube.com/watch?v=yqMwXF0Qk0Q) is highly effective for hook creation. Don't expect AI to deliver perfection, but use it to generate ideas and phrasing.\n",
            "    *   **Step 3a: Provide Context & Examples:** Give your AI assistant (e.g., ChatGPT, Claude) the core topic of your tech video. Crucially, \"provide [it] with four hooks [you] had scripted for previous videos of mine\" (or from your \"Model 10\" research that you liked). This trains the AI on your desired style and effectiveness.\n",
            "        *   **Example Prompt:** \"I'm making a YouTube video about 'How to Optimize Your Gaming PC for Max Performance.' Here are some hooks from successful tech channels I like:\n",
            "            *   'Is your gaming PC stuttering? Here's the one setting you're missing.'\n",
            "            *   'Turn your old PC into a beast – without buying new parts!'\n",
            "            *   'Don't buy a new GPU! Do THIS first for insane FPS gains.'\n",
            "            *   'The secret to smooth gameplay isn't your graphics card...'\n",
            "            *   Now, write 5-7 hook options for my video, focusing on curiosity and a clear benefit for gamers.\"\n",
            "    *   **Step 3b: Iterate and Refine:** The creator in the AI video stated, \"I liked certain components of the hook that she wrote but I wasn't sold after a few messages trying to further refine the hook I ended up just taking some of her ideas and phrasing and filled out the rest of the hook myself.\" This is key.\n",
            "        *   **Actionable:** Review the AI's suggestions. Pick out phrases, ideas, or angles you like. Ask the AI to \"refine\" specific options (e.g., \"Make this hook more urgent,\" \"Can you make it more specific to software optimization?\"). Combine the best elements from different AI suggestions with your own unique voice and deep tech knowledge.\n",
            "        *   **Human Touch:** The final hook should always be refined by you to ensure it aligns with your brand, tone, and accurately sets expectations for the video's content.\n",
            "\n",
            "---\n",
            "\n",
            "**Example Hook Creation (Tech-Based):**\n",
            "\n",
            "Let's say your new channel is about \"Smart Home Automation\" and your video is titled: \"**5 Smart Home Gadgets That Are Actually Worth It (and 3 to AVOID!)**\"\n",
            "\n",
            "**Applying the Steps:**\n",
            "\n",
            "1.  **Niche Curiosity/Adjacent Channels:** People are curious about making their homes smarter, but also wary of gimmicks or wasted money. They want convenience and efficiency. Adjacent channels might include product reviews, home improvement, or even lifestyle content featuring tech. Hooks often highlight \"time-saving,\" \"cost-saving,\" or \"avoiding mistakes.\"\n",
            "2.  **Keyword Research:** Common searches might be \"best smart home devices,\" \"smart home gadgets review,\" \"smart home mistakes,\" or specific device names. Your hook should promise value related to these.\n",
            "3.  **AI Brainstorming & Refinement:**\n",
            "\n",
            "    *   **Initial AI Prompt:** \"I'm making a video about '5 Smart Home Gadgets That Are Actually Worth It (and 3 to AVOID!).' Generate 5 hooks that grab attention and tell viewers what they'll gain.\"\n",
            "\n",
            "    *   **AI Suggestions (Example):**\n",
            "        1.  \"Thinking about smart home tech? Stop! Watch this before you buy anything.\"\n",
            "        2.  \"Your smart home could be smarter. I'll show you the must-haves and the total busts.\"\n",
            "        3.  \"Don't waste money on these 3 smart home gadgets. Here's what to buy instead.\"\n",
            "        4.  \"Is your smart home actually dumb? Discover the 5 gadgets that deliver.\"\n",
            "        5.  \"I spent $1000s on smart home tech so you don't have to. Here's what worked.\"\n",
            "\n",
            "    *   **Your Refinement:** You like elements from 1, 3, and 5. You want to emphasize both the \"worth it\" and \"avoid\" aspects.\n",
            "\n",
            "        **Final Hook:**\n",
            "        \"**Before you spend another dollar on smart home tech, listen up! I've bought and tested dozens of gadgets, and today I'm revealing the *only* 5 truly worth your money... and the 3 you absolutely need to AVOID. Your wallet (and your sanity) will thank me.**\"\n",
            "\n",
            "This hook combines a direct call to action, promises value (saving money/time), hints at insider knowledge, and creates curiosity about both the good and the bad, all while being immediately relevant to a new viewer interested in smart home tech.\n",
            "\n",
            "==== Source Videos ====\n",
            "\n",
            "- How to START & GROW a YouTube Channel in 2025 | FULL COURSE (https://www.youtube.com/watch?v=SXN9SMLWLzw) [Score: 0.8748]\n",
            "- STARTING OVER ON YOUTUBE | why I left my 22K subscribers behind (https://www.youtube.com/watch?v=b0iBT83zzBs) [Score: 1.1324]\n",
            "- How to Turn AI Into Your Personal YouTube Team (https://www.youtube.com/watch?v=yqMwXF0Qk0Q) [Score: 1.1895]\n",
            "- MEET MY YOUTUBE PRODUCER!! (YouTube Hiring Q&A) (https://www.youtube.com/watch?v=ilDaicqCfds) [Score: 1.2162]\n",
            "\n",
            "---\n",
            "\n",
            "Your question: i am a new youtube channel tech based, i am starting my own podcast. i want to make a 2 minute video introduction script.\n",
            "\n",
            "[Retrieving, reasoning...]\n",
            "\n",
            "\n",
            "==== AI's Answer ====\n",
            "\n",
            "As an advanced YouTube Channel Growth Coach, I'm excited to help you craft a compelling 2-minute introduction for your new tech podcast. Based on the provided excerpts, here's how we'll build your script:\n",
            "\n",
            "### Your 2-Minute Podcast Introduction Strategy\n",
            "\n",
            "Your goal for this 2-minute video is to act as a powerful \"roadmap\" for your podcast, clearly outlining what listeners can expect and why they should tune in.\n",
            "\n",
            "**Step 1: Nail Your Hook – The First 20 Seconds**\n",
            "According to \"how to make a killer youtube intro (to blow up your channel)\" (TOHQXdCAo5k), the first 20 seconds are critical. \"All of its potential could be wasted on the first 20 seconds.\" For a podcast, \"Fixing YOUR titles, thumbnails, & intros (again)\" (ODmlNtOedmo) advises that what people expect for an intro is \"to be teased upcoming value to know what they're going to get.\" This means starting with a compelling question or statement that immediately highlights the value your podcast offers to a tech-savvy audience.\n",
            "\n",
            "**Step 2: Structure with the \"Three Ps\"**\n",
            "\"Every video is comprised of three core elements: promise, progress, and payoff,\" as taught in \"how to make your FIRST YouTube video in 2025\" (ZWKxukGWF5U). This framework is perfect for your intro:\n",
            "\n",
            "*   **Promise:** What will your tech podcast deliver? What problem does it solve, or what curiosity does it satisfy for your audience?\n",
            "*   **Progress:** How will you deliver on that promise? What's the format? What kind of discussions, guests, or segments can listeners expect?\n",
            "*   **Payoff:** What will the audience gain by listening? What's the ultimate benefit for them?\n",
            "\n",
            "**Step 3: Keep it Concise and Focused**\n",
            "Remember, scripting is about \"creating a road map to follow so you stay clear and focused when it's time to hit record without a script it's easy to ramble lose your train of thought or leave out something important\" (\"how to make your FIRST YouTube video in 2025\" - ZWKxukGWF5U). Stick to your core message and the Three Ps to ensure you hit that 2-minute mark effectively.\n",
            "\n",
            "---\n",
            "\n",
            "### Your 2-Minute Podcast Introduction Script\n",
            "\n",
            "Here’s a script tailored for your tech podcast, incorporating the advice from the excerpts:\n",
            "\n",
            "**(Video opens with dynamic tech-related visuals – maybe quick cuts of futuristic interfaces, complex code, or intriguing gadgets. Energetic but not overwhelming background music.)**\n",
            "\n",
            "**[0:00 - 0:25] THE HOOK (Tease Upcoming Value & Promise)**\n",
            "**(You, looking directly at the camera, with an engaging, confident expression)**\n",
            "\n",
            "\"Are you tired of feeling overwhelmed by the relentless pace of tech? Do you scroll through headlines and wonder what's *really* going to change your world next year, next month, or even tomorrow? We all know tech is moving fast, but understanding its true impact, separating the hype from the breakthroughs – that’s a whole different challenge. And that's exactly why **[Your Podcast Name]** is here.\"\n",
            "\n",
            "**(Source: \"Fixing YOUR titles, thumbnails, & intros (again)\" - https://www.youtube.com/watch?v=ODmlNtOedmo, for teasing upcoming value; \"how to make a killer youtube intro (to blow up your channel)\" - https://www.youtube.com/watch?v=TOHQXdCAo5k, for the importance of the first 20 seconds.)**\n",
            "\n",
            "**[0:25 - 1:20] THE PROMISE & PROGRESS**\n",
            "\"I’m [Your Name], and on **[Your Podcast Name]**, we're cutting through the noise to bring you clarity and real insight into the tech landscape. This isn't just another podcast about gadgets; it's about understanding the *why* behind the what.\n",
            "\n",
            "Each week, we'll dive deep into the most exciting and sometimes daunting aspects of technology. We’ll cover everything from cutting-edge AI and the future of quantum computing, to the latest in cybersecurity and the ethical dilemmas shaping our digital lives. We'll break down complex concepts into digestible insights, helping you grasp the big picture without needing a computer science degree.\n",
            "\n",
            "You can expect engaging discussions, expert interviews with industry leaders, and practical takeaways you won't find anywhere else. Think of it as your weekly dose of future-proofing.\"\n",
            "\n",
            "**(Source: \"how to make your FIRST YouTube video in 2025\" - https://www.youtube.com/watch?v=ZWKxukGWF5U, for the \"Promise\" and \"Progress\" elements of the Three Ps, and the concept of scripting as a roadmap to stay clear and focused.)**\n",
            "\n",
            "**[1:20 - 1:50] THE PAYOFF**\n",
            "\"The payoff for you? You’ll not only stay ahead of the curve but also gain a deeper, more nuanced understanding of the technology shaping our world. You’ll be better equipped to make informed decisions, whether you’re a professional in the field, an aspiring innovator, or just someone fascinated by the digital frontier. Our goal is to empower you with knowledge, spark new ideas, and maybe even challenge your perspective on what's possible.\"\n",
            "\n",
            "**(Source: \"how to make your FIRST YouTube video in 2025\" - https://www.youtube.com/watch?v=ZWKxukGWF5U, for the \"Payoff\" element of the Three Ps.)**\n",
            "\n",
            "**[1:50 - 2:00] CALL TO ACTION**\n",
            "\"So, if you’re ready to explore the future of tech with us, hit that subscribe button right now, and tune into the very first episode of **[Your Podcast Name]**, coming [Day of Week / Date]! Don't miss out.\"\n",
            "\n",
            "**(Video ends with podcast logo, social media handles, and a strong call to subscribe.)**\n",
            "\n",
            "---\n",
            "\n",
            "This script provides a clear roadmap for your first video, ensuring you deliver value quickly and effectively, setting the stage for your podcast's success! Remember, \"your first YouTube video doesn't have to be perfect, it just has to exist\" (\"how to make your FIRST YouTube video in 2025\" - https://www.youtube.com/watch?v=ZWKxukGWF5U). Good luck!\n",
            "\n",
            "==== Source Videos ====\n",
            "\n",
            "- how to make your FIRST YouTube video in 2025 (https://www.youtube.com/watch?v=ZWKxukGWF5U) [Score: 0.7997]\n",
            "- Fixing YOUR titles, thumbnails, & intros (again) (https://www.youtube.com/watch?v=ODmlNtOedmo) [Score: 0.8044]\n",
            "- how to make a killer youtube intro (to blow up your channel) (https://www.youtube.com/watch?v=TOHQXdCAo5k) [Score: 0.8867]\n",
            "- how to make your FIRST YouTube video in 2025 (https://www.youtube.com/watch?v=ZWKxukGWF5U) [Score: 0.9267]\n",
            "\n",
            "---\n",
            "\n",
            "Your question: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}